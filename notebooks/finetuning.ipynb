{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/miniconda3/envs/claim-extraction/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prompt engineering https://community.openai.com/t/prompt-engineering-for-rag/621495"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fine-tuning is based on the following:\n",
    "#    A base model\n",
    "#    A base tokenizer\n",
    "#    A set of desired (input, output) pairs\n",
    "#        Importantly, there is some nuance with how the chat template is applied to the input, output pairs\n",
    "#        This notebook provides a framework for fine-tuning with a system prompt\n",
    "#            and a fixed Yes/No question with a fixed Yes/No output."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "cache_dir = \"../assets/models\"\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True, padding_side=\"left\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "AVeriTeC = pd.read_json('../data/AVeriTeC/train.json')\n",
    "AVeriTeC = AVeriTeC.rename(columns={\"claim\": \"sentence\"})\n",
    "AVeriTeC['label'] = 'Yes'\n",
    "AVeriTeC = AVeriTeC.filter(items=['sentence','label'])\n",
    "AVeriTeC.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hunter Biden had no experience in Ukraine or i...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump delivered the largest tax cuts in...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nigeria … in terms of revenue share, 20% go...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden has pledged to stop border wall construc...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After the police shooting of Jacob Blake, Gov....</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label\n",
       "0  Hunter Biden had no experience in Ukraine or i...   Yes\n",
       "1  Donald Trump delivered the largest tax cuts in...   Yes\n",
       "2  In Nigeria … in terms of revenue share, 20% go...   Yes\n",
       "3  Biden has pledged to stop border wall construc...   Yes\n",
       "4  After the police shooting of Jacob Blake, Gov....   Yes"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class BinaryClassificationTuner:\n",
    "    def __init__(self, model, tokenizer, train_dataset, messages):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.messages = messages\n",
    "\n",
    "    def train(self, epochs):\n",
    "        optimizer = Adafactor(model.parameters(), weight_decay=0.01)\n",
    "        train_dataset = self._prepare_train_data()\n",
    "        for train_instance in train_dataset:\n",
    "            for _ in range(epochs):\n",
    "                logits = model(train_instance['chat_template_input_ids'], use_cache=False)['logits']\n",
    "                loss = self._calculate_loss(logits, train_instance['label_input_ids']).mean()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(\"loss: \", loss.item())\n",
    "\n",
    "        return \n",
    "        \n",
    "    def _prepare_train_data(self):\n",
    "        \"\"\"\"\n",
    "        Returns the train dataset as a list of dictionaries, where each is a record with chat\n",
    "        \"\"\"\n",
    "        train_dataset = self.train_dataset.to_dict(orient='records')\n",
    "        train_dataset_prepared = []\n",
    "\n",
    "        for train_instance in train_dataset:\n",
    "            messages = copy.deepcopy(self.messages)\n",
    "            for message in messages:\n",
    "                if message['role'] == \"user\":\n",
    "                    message['content'] = message['content'].replace('__SENTENCE__',train_instance['sentence'])\n",
    "                    break\n",
    "                \n",
    "            chat_template_input_ids = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "            chat_template_input_ids = chat_template_input_ids[0, :-1].reshape(1,-1)\n",
    "            \n",
    "            label_input_ids = tokenizer(train_instance['label'], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=chat_template_input_ids.shape[1])['input_ids']\n",
    "            label_input_ids = torch.where(label_input_ids != tokenizer.pad_token_id, label_input_ids, -100)\n",
    "\n",
    "            train_dataset_prepared.append({'chat_template_input_ids': chat_template_input_ids,'label_input_ids': label_input_ids})\n",
    "\n",
    "        return train_dataset_prepared\n",
    "\n",
    "    def _calculate_loss(self, logits, labels):\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return cross_entropy_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Demo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI agent used to determine whether or not a sentence is a factual claim. Only respond with Yes or No\",},\n",
    "    {\"role\": \"user\", \"content\": \"Is the following sentence a factual claim? __SENTENCE__\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "bct = BinaryClassificationTuner(model, tokenizer, AVeriTeC.head(2), messages)\n",
    "bct.train(epochs=2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'role': 'system', 'content': 'You are an AI agent used to determine whether or not a sentence is a factual claim. Only respond with Yes or No'}, {'role': 'user', 'content': 'Is the following sentence a factual claim? __SENTENCE__'}, {'role': 'assistant', 'content': ''}]\n",
      "loss:  0.006558963563293219\n",
      "loss:  0.005200252402573824\n",
      "loss:  3.193731390638277e-05\n",
      "loss:  0.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a yes/no answering bot. Only respond to questions with Yes or No\",},\n",
    "    {\"role\": \"user\", \"content\": \"Is the capital of New York state New York City?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "answer = \"Yes\"\n",
    "chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)\n",
    "chat_template_input_ids = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "chat_template_input_ids = chat_template_input_ids[0, :-1].reshape(1,-1)\n",
    "\n",
    "label_tokenized = tokenizer([answer], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=chat_template_input_ids.shape[1])['input_ids']\n",
    "\n",
    "# -100 comes from the Llama documentation, recommendation for loss\n",
    "label_tokenized_fixed = torch.where(label_tokenized != tokenizer.pad_token_id, label_tokenized, -100)\n",
    "\n",
    "# You can use the following to test what the geneartion would complete\n",
    "#print(tokenizer.batch_decode(model.generate(chat_template_input_ids, max_new_tokens = 1))[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# You can use the following to test what the geneartion would complete\n",
    "# Test a before resposne\n",
    "print(tokenizer.batch_decode(model.generate(chat_template_input_ids, max_new_tokens = 1))[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = Adafactor(model.parameters(), weight_decay=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for _ in range(3):\n",
    "    logits = model(chat_template_input_ids, use_cache=False)[\"logits\"]\n",
    "    loss = calculate_loss(logits, label_tokenized_fixed).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss: \", loss.item())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tokenizer.batch_decode(model.generate(chat_template_input_ids, max_new_tokens = 1))[0])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.13.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.13.2 64-bit ('claim-extraction': conda)"
  },
  "interpreter": {
   "hash": "6b023f274101c0d83a2338600e52d2429e6f2c6f40deb617cd262127fbe3c9bb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}