{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "AVeriTeC = pd.read_json('../data/AVeriTeC/train.json')\n",
    "AVeriTeC = AVeriTeC.rename(columns={\"claim\": \"sentence\"})\n",
    "AVeriTeC['claim'] = 'Yes'\n",
    "AVeriTeC = AVeriTeC.filter(items=['sentence','claim'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "subj = pd.read_csv('../data/subj/train.tsv', sep='\\t')\n",
    "subj = subj[subj['label'] == 1].reset_index()\n",
    "subj['claim'] = 'No'\n",
    "subj = subj.filter(items=['sentence','claim'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "train_df = pd.concat([AVeriTeC,subj])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "cache_dir = \"../assets/models\"\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True, padding_side=\"left\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "# A convention that makes training easier\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a yes/no answering bot. Only respond to questions with Yes or No.\",},\n",
    "    {\"role\": \"user\", \"content\": \"Is the capital of New York state New York City?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "answer = \"No\"\n",
    "#tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#print(tokenizer.decode(tokenized_chat[0]))\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "#input_ids = input_ids[0][:-1].reshape(1, -1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        input_ids,\n",
    "        use_cache=False,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "    )[\"logits\"]\n",
    "logits\n",
    "torch.argmax(logits[-1,1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(128006)"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(tokenizer.batch_decode(model.generate(input_ids, max_new_tokens = 1), remove_special_chars=True)[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a yes/no answering bot. Only respond to questions with Answer: Yes or Answer: No \",},\n",
    "    {\"role\": \"user\", \"content\": \"Is the capital of New York state New York City?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer:\"}\n",
    "]\n",
    "answer = \"No\"\n",
    "chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)\n",
    "chat_template_input_ids = tokenizer.apply_chat_template(messages, tokenize=True, continue_final_message=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "#print(chat_template)\n",
    "\n",
    "label_tokenized = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=chat_template_input_ids.shape[1])['input_ids']\n",
    "\n",
    "# -100 comes from the Llama documentation, recommendation for loss\n",
    "label_tokenized_fixed = torch.where(label_tokenized != tokenizer.pad_token_id, label_tokenized, -100)\n",
    "#label_tokenized_fixed[:, -1] = tokenizer.pad_token_id\n",
    "label_tokenized_fixed\n",
    "\n",
    "# These should be equal\n",
    "# chat_template_input_ids.shape, labels_tokenized_fixed.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, 2360]])"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "#chat_template_input_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "print(tokenizer.batch_decode(model.generate(chat_template_input_ids, max_new_tokens = 10))[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Apr 2025\n",
      "\n",
      "You are a yes/no answering bot. Only respond to questions with Answer: Yes or Answer: No<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Is the capital of New York state New York City?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Answer: No No No No No No No No No No\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        chat_template_input_ids,\n",
    "        use_cache=True\n",
    "    )[\"logits\"]\n",
    "#logits = model(input_ids=chat_template_input_ids)[\"logits\"]\n",
    "#logits.shape\n",
    "\n",
    "torch.argmax(logits[0,-1])\n",
    "logits.argmax(axis=-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 16309, 128006,   9891,   7566,   9642,   7566,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566,   9642,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   9642,   7566,   7566,   9642,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,   7566,\n",
       "           7566,   7566,   7566,   7566,   7566]])"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "label_tokenized_fixed"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, 2360]])"
      ]
     },
     "metadata": {},
     "execution_count": 293
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "#out = model.generate(chat_template_input_ids, output_logits=True, return_dict_in_generate=True)\n",
    "#out.logits"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[6.3550, 4.1672, 5.3958,  ..., 2.9041, 2.9040, 2.9043]]),\n",
       " tensor([[12.5756,  9.9608,  4.8848,  ...,  2.6261,  2.6255,  2.6263]]))"
      ]
     },
     "metadata": {},
     "execution_count": 272
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "calculate_loss(logits, label_tokenized_fixed)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000, 44.3867])"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "optimizer = Adafactor(model.parameters(), weight_decay=0.01)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "for _ in range(3):\n",
    "    logits = model(chat_template_input_ids, use_cache=False)[\"logits\"]\n",
    "    loss = calculate_loss(logits, label_tokenized_fixed).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss: \", loss.item())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss:  0.6527453064918518\n",
      "loss:  0.0005169016076251864\n",
      "loss:  1.90331138583133e-05\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        chat_template_input_ids,\n",
    "        use_cache=True\n",
    "    )[\"logits\"]\n",
    "#logits = model(input_ids=chat_template_input_ids)[\"logits\"]\n",
    "#logits.shape\n",
    "\n",
    "#torch.argmax(logits[0,-1])\n",
    "#logits.argmax(axis=-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  791,  2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360,\n",
       "          2360,  2360,  2360,  2360,  2360,   912,   912,  2360,  2360,  2360,\n",
       "          2360,  2360,  2360,  2360,  2360,   912,   912,   912,   912,   912,\n",
       "          2360,  2360,  2360,   912,  2360,  2360,  2360,  2360,  2360,  2360,\n",
       "           912, 84343,  2360,  2360,   912, 84343,   912,  2360,  2360,  2360,\n",
       "          2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360,\n",
       "          2360,  2360,  2360,  2360,  2360,  2360,  2360,  2360]])"
      ]
     },
     "metadata": {},
     "execution_count": 297
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.13.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.13.2 64-bit ('claim-extraction': conda)"
  },
  "interpreter": {
   "hash": "6b023f274101c0d83a2338600e52d2429e6f2c6f40deb617cd262127fbe3c9bb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}